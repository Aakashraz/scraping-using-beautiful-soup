import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class TranscriptSpider(CrawlSpider):
    name = "transcript"
    # custom_settings = {
    #     'FEED_EXPORT_ENCODING': 'utf-8',
    # }
    allowed_domains = ["subslikescript.com"]
    # start_urls = ["https://subslikescript.com/movies_letter-X"]     # for less no of pages to crawl
    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:129.0) Gecko/20100101 Firefox/129.0'

    # This is for the initial requests to kick off the spider and load the first page.
    # The spider starts by sending a request to https://subslikescript.com/movies_letter-X
    # with the custom User-Agent set in start_requests()
    def start_requests(self):
        yield scrapy.Request(
            url='https://subslikescript.com/movies_letter-X',
            headers={
                'User-Agent': self.user_agent
            },
        )

    # Key Concepts of rules:
    #
    #     LinkExtractor: Identifies which links to follow based on patterns (allow and deny).
    #     callback: Specifies the method to call when certain links are followed (e.g., parse_item).
    #     Rules: Define how the spider behaves when encountering links on the page (whether to follow or extract data).
    rules = (
        Rule(LinkExtractor(restrict_xpaths="//ul[@class='scripts-list']/li/a"), callback="parse_item", follow=True,
             process_request='set_user_agent'),
        Rule(LinkExtractor(restrict_xpaths="(//a[@rel='next'])[1]"), follow=True, process_request='set_user_agent'),
    )

    # Extracting the data from matching websites in the LinkExtractor is:
    #     Recursive Behavior: The process of following links and handling responses is recursive.
    #     It involves continuously fetching new pages, extracting data, and following further links.
    #
    #     Loop-like Process: Although not a literal loop, the process behaves similarly by continually
    #     handling new pages and links until the rules and constraints are met or there are no more links to follow.
    #
    # This systematic approach allows Scrapy to efficiently crawl and scrape large and complex websites.

    # Understanding the process_request inside the Rule:
    # The process_request method lets you modify each request before it is sent, enabling you to:
    #
    #     Set a custom User-Agent for every request.
    #     Add custom headers or cookies.
    #     Modify or log the request in some other way before it is sent.

    # Why This Is Important
    # Some websites block Scrapy or other bots, so setting a browser-like User-Agent helps
    # your spider avoid detection and reduces the chances of being blocked by the website.
    def set_user_agent(self, request, spider):
        # request: This is the Request object generated by Scrapy when it encounters a link that matches
        # the LinkExtractor rule. You can modify this request before it's sent to the server.

        # spider: This is the spider object itself, giving you access to the spider's attributes, methods, and settings.
        # For instance, in your method, spider could be used to access the spider's attributes such as self.user_agent.
        request.headers['User-Agent'] = self.user_agent
        return request

    # This is for all subsequent requests generated automatically by Scrapy when following links
    # (like the "next" page). You want to ensure that all those subsequent requests also use the correct User-Agent.

    # Method to extract the full_scripts without the first line
    # BUT THIS METHOD IS MAKING THE SYSTEM OUTPUT VERY SLOW BY SEPARATING WITH LETTERS AS: 'w e a r e h a p p y'
    # HENCE NOT USING AT THE MOMENT
    def extract_transcript(self, response):
        # Extract all text nodes, including the first line
        transcript_nodes = response.xpath(
            './div[@class="full-script"]/text() | ./div[@class="full-script"]/br/following-sibling::text()').getall()
        # Remove empty string and strip whitespace
        transcript_lines = [line.strip() for line in transcript_nodes if line.strip()]
        # The above line will: Process each line and remove leading/trailing whitespace.
        # Filter out lines that only contain whitespace (like " " and "\n").

        # skip the first line and join the rest
        transcript = '\n'.join(transcript_lines[2:])
        return transcript

    def parse_item(self, response):
        print(
            f'type of response.encoding:{type(response.encoding)}, {response.encoding}---------------------------')  # This should print <class 'str'> 'utf-8'

        # Explicitly decode the response body
        # response = response.replace
        # Force encoding to UTF-8
        # response.replace(encoding='utf-8')

        article = response.xpath('//article[@class="main-article"]')
        # print(response.url)
        # This converts the byte string back to a regular string (str) in Python, which will not display the b prefix when printed.
        user_agent_str = response.request.headers['User-Agent'].decode('utf-8')

        # to change the transcript_list into transcript_string
        transcript_from_method = self.extract_transcript(article)
        # print(f'transcript sample:---------------------{transcript_from_method}')

        # this method includes unwanted first line of strings from the full transcripts of the movie
        # transcript_list = article.xpath('./div[@class="full-script"]/text()').getall()
        # transcript_string = ' '.join(transcript_list)

        yield {
            'title': article.xpath('./h1/text()').get().split('-')[0],
            'plot': article.xpath('./p/text()').get(),
            'full_script': transcript_from_method,
            'url': response.url,
            # to check the user-agent
            # 'USER-AGENT': user_agent_str,
        }

# Why You Need Both methods mentioned above as, start_requests() and set_user_agent();
#
#     start_requests(): This is for the initial requests to kick off the spider and load the first page.
#     These are explicitly defined by you.
#
#     process_request() (set_user_agent): This is for all subsequent requests generated automatically
#     by Scrapy when following links (like the "next" page). You want to ensure that all those subsequent
#     requests also use the correct User-Agent.
